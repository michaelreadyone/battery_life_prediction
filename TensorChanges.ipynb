{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import get_train_test, setup_seed\n",
    "\n",
    "seed=0\n",
    "setup_seed(seed)\n",
    "\n",
    "K = 4\n",
    "Rated_Capacity = 1.1\n",
    "feature_size = 6\n",
    "feature_num = K\n",
    "dropout = 0.0\n",
    "epochs = 500\n",
    "nhead = 1\n",
    "hidden_dim = 16\n",
    "num_layers = 3\n",
    "lr = 0.001  \n",
    "weight_decay = 0.0\n",
    "noise_level = 0.0\n",
    "alpha = 0.1\n",
    "metric = 're'\n",
    "seed = 0\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape: (2886, 6), train_y.shape: (2886,)\n"
     ]
    }
   ],
   "source": [
    "Battery = np.load('datasets/CALCE/CALCE.npy', allow_pickle=True)\n",
    "Battery = Battery.item() # A dict with cell name as key and dataframe as value\n",
    "name = 'CS2_35'\n",
    "train_x, train_y, train_data, test_data = get_train_test(Battery, name, feature_size)\n",
    "print(f'train_x.shape: {train_x.shape}, train_y.shape: {train_y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after reshape\n",
      "x.shape: (2886, 1, 6)\n",
      "y.shape: (2886, 1)\n",
      "after repeat\n",
      "x.shape: torch.Size([2886, 4, 6])\n",
      "y.shape: torch.Size([2886, 1])\n"
     ]
    }
   ],
   "source": [
    "x = np.reshape(train_x/Rated_Capacity,(-1, 1, feature_size)).astype(np.float32)\n",
    "y = np.reshape(train_y/Rated_Capacity,(-1,1)).astype(np.float32) \n",
    "print(f'after reshape\\nx.shape: {x.shape}\\ny.shape: {y.shape}')\n",
    "\n",
    "x, y = torch.from_numpy(x).to(device), torch.from_numpy(y).to(device)\n",
    "x = x.repeat(1, K, 1)\n",
    "print(f'after repeat\\nx.shape: {x.shape}\\ny.shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, feature_len, feature_size, dropout=0.0):\n",
    "        # feature_len: max length of sequence, feature_size: embedding size(d_model)\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(feature_len, feature_size)\n",
    "        position = torch.arange(0, feature_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, feature_size, 2).float() * (-math.log(10000.0) / feature_size))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # print(f'pe.shape: {pe.shape}')\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe\n",
    "\n",
    "  \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, feature_size, hidden_dim, feature_num, num_layers, nhead):\n",
    "        super(Transformer, self).__init__()\n",
    "        half_feature_size = int(feature_size / 2)\n",
    "        print(f'dropout: {dropout}')\n",
    "        \n",
    "        self.autoencoder = nn.Linear(feature_size, half_feature_size)\n",
    "        self.pe = PositionalEncoding(half_feature_size, feature_num)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=feature_num, nhead=nhead, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)\n",
    "        self.layers = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.lm_head = nn.Linear(feature_num*half_feature_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        print(f'input x.shape: {x.shape}')\n",
    "        print(x)\n",
    "        x = self.autoencoder(x)\n",
    "        print(f'after autoencoder, x.shape: {x.shape}')\n",
    "        print(x)\n",
    "        x = x.reshape(B, -1, T)\n",
    "        print(f'after reshape before positional encoding, x.shape: {x.shape}')\n",
    "        print(x)\n",
    "        x = self.pe(x)\n",
    "        print(f'after positional encoding, x.shape: {x.shape}')\n",
    "        print(x)\n",
    "        x = self.layers(x)\n",
    "        print(f'after transformer layers, x.shape: {x.shape}')\n",
    "        # print(x)\n",
    "        x = x.reshape(B, -1)\n",
    "        print(f'after reshape before lm_head, x.shape: {x.shape}')\n",
    "        # print(x)\n",
    "        x = self.lm_head(x)\n",
    "        print(f'after lm_head, x.shape: {x.shape}')\n",
    "        # print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout: 0.0\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(feature_size, hidden_dim, feature_num, num_layers, nhead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input x.shape: torch.Size([2886, 4, 6])\n",
      "tensor([[[1.0307, 1.0304, 1.0298, 1.0246, 1.0243, 1.0221],\n",
      "         [1.0307, 1.0304, 1.0298, 1.0246, 1.0243, 1.0221],\n",
      "         [1.0307, 1.0304, 1.0298, 1.0246, 1.0243, 1.0221],\n",
      "         [1.0307, 1.0304, 1.0298, 1.0246, 1.0243, 1.0221]],\n",
      "\n",
      "        [[1.0304, 1.0298, 1.0246, 1.0243, 1.0221, 1.0136],\n",
      "         [1.0304, 1.0298, 1.0246, 1.0243, 1.0221, 1.0136],\n",
      "         [1.0304, 1.0298, 1.0246, 1.0243, 1.0221, 1.0136],\n",
      "         [1.0304, 1.0298, 1.0246, 1.0243, 1.0221, 1.0136]],\n",
      "\n",
      "        [[1.0298, 1.0246, 1.0243, 1.0221, 1.0136, 1.0082],\n",
      "         [1.0298, 1.0246, 1.0243, 1.0221, 1.0136, 1.0082],\n",
      "         [1.0298, 1.0246, 1.0243, 1.0221, 1.0136, 1.0082],\n",
      "         [1.0298, 1.0246, 1.0243, 1.0221, 1.0136, 1.0082]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.3500, 0.3417, 0.3417, 0.3333, 0.3333, 0.3333],\n",
      "         [0.3500, 0.3417, 0.3417, 0.3333, 0.3333, 0.3333],\n",
      "         [0.3500, 0.3417, 0.3417, 0.3333, 0.3333, 0.3333],\n",
      "         [0.3500, 0.3417, 0.3417, 0.3333, 0.3333, 0.3333]],\n",
      "\n",
      "        [[0.3417, 0.3417, 0.3333, 0.3333, 0.3333, 0.3250],\n",
      "         [0.3417, 0.3417, 0.3333, 0.3333, 0.3333, 0.3250],\n",
      "         [0.3417, 0.3417, 0.3333, 0.3333, 0.3333, 0.3250],\n",
      "         [0.3417, 0.3417, 0.3333, 0.3333, 0.3333, 0.3250]],\n",
      "\n",
      "        [[0.3417, 0.3333, 0.3333, 0.3333, 0.3250, 0.3250],\n",
      "         [0.3417, 0.3333, 0.3333, 0.3333, 0.3250, 0.3250],\n",
      "         [0.3417, 0.3333, 0.3333, 0.3333, 0.3250, 0.3250],\n",
      "         [0.3417, 0.3333, 0.3333, 0.3333, 0.3250, 0.3250]]])\n",
      "after autoencoder, x.shape: torch.Size([2886, 4, 3])\n",
      "tensor([[[ 0.1511, -0.1350,  0.2649],\n",
      "         [ 0.1511, -0.1350,  0.2649],\n",
      "         [ 0.1511, -0.1350,  0.2649],\n",
      "         [ 0.1511, -0.1350,  0.2649]],\n",
      "\n",
      "        [[ 0.1487, -0.1363,  0.2613],\n",
      "         [ 0.1487, -0.1363,  0.2613],\n",
      "         [ 0.1487, -0.1363,  0.2613],\n",
      "         [ 0.1487, -0.1363,  0.2613]],\n",
      "\n",
      "        [[ 0.1471, -0.1390,  0.2590],\n",
      "         [ 0.1471, -0.1390,  0.2590],\n",
      "         [ 0.1471, -0.1390,  0.2590],\n",
      "         [ 0.1471, -0.1390,  0.2590]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1649, -0.1259,  0.1748],\n",
      "         [-0.1649, -0.1259,  0.1748],\n",
      "         [-0.1649, -0.1259,  0.1748],\n",
      "         [-0.1649, -0.1259,  0.1748]],\n",
      "\n",
      "        [[-0.1655, -0.1251,  0.1736],\n",
      "         [-0.1655, -0.1251,  0.1736],\n",
      "         [-0.1655, -0.1251,  0.1736],\n",
      "         [-0.1655, -0.1251,  0.1736]],\n",
      "\n",
      "        [[-0.1658, -0.1282,  0.1713],\n",
      "         [-0.1658, -0.1282,  0.1713],\n",
      "         [-0.1658, -0.1282,  0.1713],\n",
      "         [-0.1658, -0.1282,  0.1713]]], grad_fn=<ViewBackward0>)\n",
      "after reshape before positional encoding, x.shape: torch.Size([2886, 3, 4])\n",
      "tensor([[[ 0.1511, -0.1350,  0.2649,  0.1511],\n",
      "         [-0.1350,  0.2649,  0.1511, -0.1350],\n",
      "         [ 0.2649,  0.1511, -0.1350,  0.2649]],\n",
      "\n",
      "        [[ 0.1487, -0.1363,  0.2613,  0.1487],\n",
      "         [-0.1363,  0.2613,  0.1487, -0.1363],\n",
      "         [ 0.2613,  0.1487, -0.1363,  0.2613]],\n",
      "\n",
      "        [[ 0.1471, -0.1390,  0.2590,  0.1471],\n",
      "         [-0.1390,  0.2590,  0.1471, -0.1390],\n",
      "         [ 0.2590,  0.1471, -0.1390,  0.2590]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1649, -0.1259,  0.1748, -0.1649],\n",
      "         [-0.1259,  0.1748, -0.1649, -0.1259],\n",
      "         [ 0.1748, -0.1649, -0.1259,  0.1748]],\n",
      "\n",
      "        [[-0.1655, -0.1251,  0.1736, -0.1655],\n",
      "         [-0.1251,  0.1736, -0.1655, -0.1251],\n",
      "         [ 0.1736, -0.1655, -0.1251,  0.1736]],\n",
      "\n",
      "        [[-0.1658, -0.1282,  0.1713, -0.1658],\n",
      "         [-0.1282,  0.1713, -0.1658, -0.1282],\n",
      "         [ 0.1713, -0.1658, -0.1282,  0.1713]]], grad_fn=<ViewBackward0>)\n",
      "after positional encoding, x.shape: torch.Size([2886, 3, 4])\n",
      "tensor([[[ 0.1511,  0.8650,  0.2649,  1.1511],\n",
      "         [ 0.7065,  0.8052,  0.1611,  0.8650],\n",
      "         [ 1.1742, -0.2650, -0.1150,  1.2647]],\n",
      "\n",
      "        [[ 0.1487,  0.8637,  0.2613,  1.1487],\n",
      "         [ 0.7052,  0.8016,  0.1587,  0.8636],\n",
      "         [ 1.1706, -0.2675, -0.1163,  1.2611]],\n",
      "\n",
      "        [[ 0.1471,  0.8610,  0.2590,  1.1471],\n",
      "         [ 0.7024,  0.7993,  0.1571,  0.8609],\n",
      "         [ 1.1683, -0.2690, -0.1190,  1.2588]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1649,  0.8741,  0.1748,  0.8351],\n",
      "         [ 0.7155,  0.7151, -0.1549,  0.8740],\n",
      "         [ 1.0840, -0.5810, -0.1059,  1.1746]],\n",
      "\n",
      "        [[-0.1655,  0.8749,  0.1736,  0.8345],\n",
      "         [ 0.7164,  0.7139, -0.1555,  0.8749],\n",
      "         [ 1.0829, -0.5816, -0.1051,  1.1734]],\n",
      "\n",
      "        [[-0.1658,  0.8718,  0.1713,  0.8342],\n",
      "         [ 0.7133,  0.7116, -0.1558,  0.8718],\n",
      "         [ 1.0806, -0.5819, -0.1082,  1.1711]]], grad_fn=<AddBackward0>)\n",
      "after transformer layers, x.shape: torch.Size([2886, 3, 4])\n",
      "after reshape before lm_head, x.shape: torch.Size([2886, 12])\n",
      "after lm_head, x.shape: torch.Size([2886, 1])\n"
     ]
    }
   ],
   "source": [
    "out = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get data [B,T] from raw data\n",
    "- Repeat T for k time and get [B, K, T]\n",
    "- Linear Map T to T/2 get [B, K, T/2]\n",
    "  - There is no sequence info in T/2?\n",
    "- reshape [not transpose] to [B, T/2, K] and then pass to Transformer layer\n",
    "  - The sequence now doesn't make any sense\n",
    "- go through lm_head, return one value at dimension T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
