- remove uncessary layers from paper model [Done]
    - remove Relu in Autoencoder
- replace load from np to load from csv [Done]
- write tensor shape change in rul model [Done]
    - see TensorChanges.ipynb file


Next Steps
- add data viz to data for better transparecy
- test common encode-decode transformer to time series prediction[Done]
- why auto-regressive couldn't have loss decrease
- quantization model?


- think about a scope/journal to publish the paper
    - Betty be PI?
        - Find another student?
    - based on matr dataset
    - new postional encoding
    - compare to all other results


- Use normal transformer for prediction from a youtube tutorial[Done]
    - Also have reasonable loss decrease, even though a little slower than paper wired model
- Use cap/volt data per cycle to enhance dataset, replace the K in the paper model, to see if this increase the performance

- find latest paper use this dataset and their results.
- Another ai tool for paper research

- try auto-regressive loss should decline, de-bug

- quantization try use the large dataset, or try the 4 cells data first.

- Will use matr data for more cells

感觉paper方法的model train的更快一点，打印一下看看有多少weight吧，

- 又有一个大胆的想法，让模型自己去自由组合各种nn，然后去比较各个组合的效果，从而进行自我迭代。感觉可以发nature 了。



Now doing:

之前的研究是，为什么paper的方法loss下降的快。

使用16个cell records上的feature来对标原来paper的方法，即重复16次的方法。所以这个feature的结构是什么？

发现summary里的disc cap和records里的disc cap不一样，怎么破

那就用records里面的cycle life作为标准，重新计算，再做成module，即使后面要改回cycle的数据，也方便操作。



写一个function 计算RE，并且画出预测电池寿命和实际寿命

在normal trasformer的module里， 加入了generate方法，和plot的方法，加载epoch500次的weights后，通过plot生成的cycle life图看上去是合理的。

然而，在paper transformer里，我把相同的functioin migrate过去后，发现有bug，通过claude code的debug后，发现input sequence只有16，并且生成的数据看上去也有问题。感觉应该是transformer过程中，paper layer里奇怪的tensor处理方式导致的。现在的目标就是看看为什么input sequence是16而不是64，而且为什么16个value都是一样的。

重新写一个自己的系统来处理数据，训练数据，推理数据，其中就可以换paper的模型和传统的模型。

训练数据的准备已经搞好，
接下来写模型，先写传统模型，