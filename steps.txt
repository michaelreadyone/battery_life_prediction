- remove uncessary layers from paper model [Done]
    - remove Relu in Autoencoder
- replace load from np to load from csv [Done]
- write tensor shape change in rul model [Done]
    - see TensorChanges.ipynb file


Next Steps
- add data viz to data for better transparecy
- test common encode-decode transformer to time series prediction[Done]
- why auto-regressive couldn't have loss decrease
- quantization model?


- think about a scope/journal to publish the paper
    - Betty be PI?
        - Find another student?
    - based on matr dataset
    - new postional encoding
    - compare to all other results


- Use normal transformer for prediction from a youtube tutorial[Done]
    - Also have reasonable loss decrease, even though a little slower than paper wired model
- Use cap/volt data per cycle to enhance dataset, replace the K in the paper model, to see if this increase the performance

- find latest paper use this dataset and their results.
- Another ai tool for paper research

- try auto-regressive loss should decline, de-bug

- quantization try use the large dataset, or try the 4 cells data first.

- Will use matr data for more cells

感觉paper方法的model train的更快一点，打印一下看看有多少weight吧，

- 又有一个大胆的想法，让模型自己去自由组合各种nn，然后去比较各个组合的效果，从而进行自我迭代。感觉可以发nature 了。



Now doing:

之前的研究是，为什么paper的方法loss下降的快。

使用16个cell records上的feature来对标原来paper的方法，即重复16次的方法。所以这个feature的结构是什么？

发现summary里的disc cap和records里的disc cap不一样，怎么破

那就用records里面的cycle life作为标准，重新计算，再做成module，即使后面要改回cycle的数据，也方便操作。



写一个function 计算RE，并且画出预测电池寿命和实际寿命